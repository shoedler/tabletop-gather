{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Board Game Geek Data Scraper\n",
    "\n",
    "The boardgame data are scraped from the BoardGameGeek XML 2.0 API. There is a limit on how many items per request can be fetched but no call limit in a given time. The scraper always requests 100 boardgames at once and stores them in a CSV.\n",
    "\n",
    "The whole scraping takes about 1h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import itertools\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import html\n",
    "import uuid\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable from 1 to 43098 comma separated\n",
    "numbers = ','.join(str(i) for i in range(1, 43099))\n",
    "\n",
    "# Split numbers into chunks of n\n",
    "def grouper(iterable, n):\n",
    "    args = [iter(iterable)] * n\n",
    "    return itertools.zip_longest(*args)\n",
    "\n",
    "id_chunks = grouper(numbers.split(','), 100)\n",
    "\n",
    "# Open CSV file for writing\n",
    "with open('bgg_data.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Name', 'Image', 'Description', 'Min Players', 'Max Players', 'Year Published', 'Min Playtime', 'Max Playtime'])\n",
    "\n",
    "    # Loop through each chunk of ids\n",
    "    for chunk in id_chunks:\n",
    "        # Remove any None values from the chunk\n",
    "        chunk = [id for id in chunk if id is not None]\n",
    "\n",
    "        # Join the ids back into a comma-separated string\n",
    "        ids = ','.join(chunk)\n",
    "\n",
    "        # Send GET request to API endpoint for the current chunk of ids\n",
    "        response = requests.get(f'https://boardgamegeek.com/xmlapi2/thing?type=boardgame&id={ids}')\n",
    "\n",
    "        # Parse response content\n",
    "        root = ET.fromstring(response.content)\n",
    "\n",
    "        # Loop through each item in the response and write the requested attributes to the CSV file\n",
    "        for item in root.findall('item'):\n",
    "            name = item.find('name[@type=\"primary\"]').attrib['value'] if item.find('name[@type=\"primary\"]') is not None else ''\n",
    "            image = item.find('image').text if item.find('image') is not None else ''\n",
    "            description = item.find('description').text if item.find('description') is not None else ''\n",
    "            minplayers = item.find('minplayers').attrib['value'] if item.find('minplayers') is not None else ''\n",
    "            maxplayers = item.find('maxplayers').attrib['value'] if item.find('maxplayers') is not None else ''\n",
    "            yearpublished = item.find('yearpublished').attrib['value'] if item.find('yearpublished') is not None else ''\n",
    "            minplaytime = item.find('minplaytime').attrib['value'] if item.find('minplaytime') is not None else ''\n",
    "            maxplaytime = item.find('maxplaytime').attrib['value'] if item.find('maxplaytime') is not None else ''\n",
    "            writer.writerow([name, image, description, minplayers, maxplayers, yearpublished, minplaytime, maxplaytime])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Insert Generator\n",
    "\n",
    "After the CSV file is created, we can use the following Python script to generate SQL INSERT statements for each row in the CSV file. AS there is a limit on flyway migration script size, we spilt these up into 10000 insert statements per script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the data from the CSV file\n",
    "with open('bgg_data.csv', mode='r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # Skip the header row\n",
    "\n",
    "    # Open SQL file for writing\n",
    "    with open('insert_data.sql', mode='w', encoding='utf-8') as sql_file:\n",
    "        # Loop through each row in the CSV file\n",
    "        for row in reader:\n",
    "            # Generate UUID based on the name\n",
    "            name = row[0]\n",
    "            id = str(uuid.uuid5(uuid.NAMESPACE_DNS, name))\n",
    "\n",
    "            # Generate current timestamp for date_created and last_updated fields\n",
    "            current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            # Extract the values from the row\n",
    "            name, image, description, minplayers, maxplayers, *_ = row  # Use * to capture additional values\n",
    "\n",
    "            # Translate special values in name\n",
    "            name = html.unescape(name)\n",
    "\n",
    "            # Write the insert statement to the SQL file\n",
    "            sql_file.write(f\"INSERT INTO games (id, name, description, min_player, max_player, image_url, date_created, last_updated) VALUES ('{id}', '{name}', $${description}$$, {minplayers}, {maxplayers}, '{image}', '{current_time}', '{current_time}');\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
